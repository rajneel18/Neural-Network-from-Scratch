Neural Network from Scratch
Overview

This project involves implementing a simple neural network from scratch using Python, without the aid of any deep learning libraries like TensorFlow or PyTorch. The goal is to understand the core concepts of neural networks, including forward and backward propagation, activation functions, loss functions, and optimization techniques.
Features

    Forward Propagation: Implemented the flow of data through the network from the input to the output layer.
    Backward Propagation: Developed the backpropagation algorithm to adjust the weights based on the error between predicted and actual output.
    Activation Functions: Used commonly used activation functions like ReLU and Sigmoid.
    Loss Functions: Implemented Mean Squared Error (MSE) and Cross-Entropy Loss functions to quantify prediction accuracy.
    Optimization: Applied Gradient Descent and Adam Optimizer to minimize the loss and improve model performance.

Requirements

    Python 3.x
    NumPy (for matrix operations)
